text(0.8,1, c("", "All ecoregions", "Mountainous ecoregions", "Null Model")[i], font=2,cex=1.4, srt=90)
} # end of i loop
for(RCP in c("RCP45", "RCP85")){
novelty <- qchi(get(paste("NN.chi.proj.IDWavg.BaseCase", RCP, sep=".")),1)
novelty[which(NN.chi.proj.IDWavg==1)] <- max(novelty[is.finite(novelty)])
novelty <- novelty[-which(teow.NAnaec8$ECO_NAME=="Lake")]
par(mar=c(0,0,0,0))
plot(1, type="n", axes=F, xlab="", ylab="")
text(1,1, if(RCP=="RCP45") {"RCP4.5"} else "RCP8.5", font=2,cex=1.5)
if(RCP=="RCP45") {par(mar=c(1,2,0,0), mgp=c(1.25,0.25,0))
} else par(mar=c(1,1,0,1), mgp=c(1.25,0.25,0))
x <- novelty[which(novelty<xlim)]
y <- el.above[which(novelty<xlim)]
if(RCP=="RCP45") {plot(1, col="white", xlim=c(0,xlim), ylim=range(y), tck=0.02, main="", xaxt="n", xaxs="i", yaxs="i", xlab="", ylab="", las=2)
} else plot(1, col="white", xlim=c(0,xlim), ylim=range(y), tck=0.02, main="", xaxt="n", yaxt="n", xaxs="i", yaxs="i", xlab="", ylab="")
points(x, y, col = alpha(densCols(x, y, colramp=colorRampPalette(brewer.pal(9, "Blues")[-(1:2)])),1), pch = 16, cex=0.7)
par(font=2); legend("topright", legend=if(RCP=="RCP45") "(a)" else "(b)", cex=1.2, bty="n"); par(font=1)
z.ref <- kde2d(x, y, n=100, h=c(diff(range(x))/15,diff(range(y))/15))
pp <- array()
for (i in 1:length(x)){
z.x <- max(which(z.ref$x < x[i]))
z.y <- max(which(z.ref$y < y[i]))
pp[i] <- z.ref$z[z.x, z.y]
}
confidencebound <- quantile(pp, c(0.01, 0.05, 0.25, 0.5), na.rm = TRUE)
contour(z.ref, levels=confidencebound, labels=c("99%","95%","75%","50%"), labcex = 0.5, add=TRUE)
box()
print("step1")
sigma.max <- aggregate(novelty, by=list(teow.NoLake$ECO_NAME), FUN=max)
sigma.max$x[!is.finite(sigma.max$x)] <- 9
threshold.range <- 1000
select.range <- which(teow.NoLake$ECO_NAME%in%el.range$Group.1[which(el.range$x>threshold.range)])
x <- novelty[select.range][which(novelty[select.range]<xlim)]
y <- el.position[select.range][which(novelty[select.range]<xlim)]
if(RCP=="RCP45") {plot(1, col="white", xlim=c(0,xlim), ylim=range(y), tck=0.02, main="", xaxt="n", xaxs="i", yaxs="i", xlab="", ylab="", las=2)
} else plot(1, col="white", xlim=c(0,xlim), ylim=range(y), tck=0.02, main="", xaxt="n", yaxt="n", xaxs="i", yaxs="i", xlab="", ylab="")
points(x, y, col = alpha(densCols(x, y, colramp=colorRampPalette(brewer.pal(9, "Blues")[-(1:3)])),1), pch = 16, cex=0.7)
par(font=2); legend("topright", legend=if(RCP=="RCP45") "(c)" else "(d)", cex=1.4, bty="n"); par(font=1)
z.ref <- kde2d(x, y, n=100, h=c(diff(range(x))/15,diff(range(y))/15))
pp <- array()
for (i in 1:length(x)){
z.x <- max(which(z.ref$x < x[i]))
z.y <- max(which(z.ref$y < y[i]))
pp[i] <- z.ref$z[z.x, z.y]
}
confidencebound <- quantile(pp, c(0.01, 0.05, 0.25, 0.5), na.rm = TRUE)
contour(z.ref, levels=confidencebound, labels=c("99%","95%","75%","50%"), labcex = 0.5, add=TRUE)
box()
print("step2")
#Null Model
if(RCP=="RCP45") {par(mar=c(2,2,0,0), mgp=c(1.25,0.25,0))
} else par(mar=c(2,1,0,1), mgp=c(1.25,0.25,0))
y <- sample(y, length(y))
if(RCP=="RCP45") {plot(1, col="white", xlim=c(0,xlim), ylim=range(y), tck=0.02, main="", xaxt="n", xaxs="i", yaxs="i", xlab="", ylab="", las=2)
} else plot(1, col="white", xlim=c(0,xlim), ylim=range(y), tck=0.02, main="", yaxt="n", xaxs="i", yaxs="i", xlab="", ylab="")
axis(1, at=seq(0,8,2), labels=seq(0,8,2), tck=0.02)
points(x, y, col = alpha(densCols(x, y, colramp=colorRampPalette(brewer.pal(9, "Blues")[-(1:3)])),1), pch = 16, cex=0.7)
par(font=2); legend("topright", legend=if(RCP=="RCP45") "(e)" else "(f)", cex=1.4, bty="n"); par(font=1)
box()
z.ref <- kde2d(x, y, n=100, h=c(diff(range(x))/15,diff(range(y))/15))
pp <- array()
for (i in 1:length(x)){
z.x <- max(which(z.ref$x < x[i]))
z.y <- max(which(z.ref$y < y[i]))
pp[i] <- z.ref$z[z.x, z.y]
}
confidencebound <- quantile(pp, c(0.01, 0.05, 0.25, 0.5), na.rm = TRUE)
contour(z.ref, levels=confidencebound, labels=c("99%","95%","75%","50%"), labcex = 0.5, add=TRUE)
print("step3")
}
par(mar=c(0,0,0,0))
plot(1, type="n", axes=F, xlab="", ylab="")
text(1,1, "Novelty (sigma dissimilarity of best analog)", font=2,cex=1.4)
dev.off()
par(mfrow=c(1,1))
#####Post-processing of climate novelty maps
### NB: THE STRATIFICATION TO GENERATE REPRESENTATIVE LOCATIONS WAS DONE WITH UN-TRANSFORMED DEGREE DAYS AND CMD. THIS WILL NEED TO BE UPDATED FOR THE FINAL ANALYSIS.
models <-  c("ACCESS1-0","CanESM2","CCSM4","CESM1-CAM5","CNRM-CM5","CSIRO-Mk3-6-0", "GFDL-CM3","GISS-E2R", "GlobalMean","HadGEM2-ES", "INM-CM4", "IPSL-CM5A-MR", "MIROC-ESM", "MIROC5", "MPI-ESM-LR","MRI-CGCM3")
options(digits=22)  #increase the defualt signif digits to 22 from 7.
library(scales)
library(MASS)   #contains the lda; also eqscplot(x,y) function, which plots axes of equal scale
library(stats)
library(rgl)
library(RColorBrewer)
library(FNN)
library(igraph)
library(raster)
library(maps)
library(mapdata)
library(maptools)
library(sp)
library(colorRamps)
library(rgdal)
library(rgeos)
library(GISTools)
require(adehabitatLT)   #provides calculation of the chi distribution
################
### Other spatial data
################
setwd("C:\\Users\\Colin\\Documents\\Masters\\Research\\SpatialData\\Boundaries")
###country boundaries
# ORIGIONAL SOURCE: http://www.diva-gis.org/gdata
countries <- readOGR(dsn="countries", layer='countries')
countries.NA <- countries[grep("Canada|United States|Mexico", countries$COUNTRY),]
# plot(countries[grep("Canada|United States|Mexico", countries$COUNTRY),])
# plot(countries.NA,xlim=c(-125.75,-125.5),ylim=c(49,49.5))
####### create a polygon mask for North America.
my_box = as(extent(-179, -50, -20, 84), "SpatialPolygons")      		# convert extent box to shapefile (rectangle)
cont.NA <- unionSpatialPolygons(countries.NA, rep(1,length(countries.NA$OBJECTID)))
cont.NA.g <- gSimplify(cont.NA, tol=0.01, topologyPreserve=TRUE)
proj4string(my_box) = projection(cont.NA)				# assign spatial projection to extent object
mask.NA <- gDifference(my_box, cont.NA.g)
projection(mask.NA)  #verify latlong projection of the study area boundary
P4S.NAEC <- CRS(projection(dem))   #establish the projection of the dem
mask.NAEC <- spTransform(mask.NA, P4S.NAEC) #reproject the countries polygons
P4S.latlon <- CRS("+proj=longlat +datum=WGS84")
### admin boundaries
bdy.can1 <- readOGR("CAN_adm",'CAN_adm1')
bdy.usa1 <- readOGR("USA_adm",'USA_adm1')
# bdy.mex1 <- readOGR("MEX_adm",'MEX_adm1')
bdy.can1 <- spTransform(bdy.can1, P4S.NAEC) #reproject the countries polygons
bdy.usa1 <- spTransform(bdy.usa1, P4S.NAEC) #reproject the countries polygons
# bdy.mex1 <- spTransform(bdy.mex1, P4S.NAEC) #reproject the countries polygons
bdy.can1 <- gSimplify(bdy.can1, tol=1000, topologyPreserve=TRUE) #generalize the linework
bdy.usa1 <- gSimplify(bdy.usa1, tol=1000, topologyPreserve=TRUE) #generalize the linework
# remove small polygons from mask and boundaries
areas <- lapply(mask.NAEC@polygons, function(x) sapply(x@Polygons, function(y) y@area))
bigpolys <- lapply(areas, function(x) which(x > 10000000))
for(i in 1:length(bigpolys)){
if(length(bigpolys[[i]]) >= 1 && bigpolys[[i]][1] >= 1){
mask.NAEC@polygons[[i]]@Polygons <- mask.NAEC@polygons[[i]]@Polygons[bigpolys[[i]]]
mask.NAEC@polygons[[i]]@plotOrder <- 1:length(mask.NAEC@polygons[[i]]@Polygons)
}
}
areas <- lapply(bdy.can1@polygons, function(x) sapply(x@Polygons, function(y) y@area))
bigpolys <- lapply(areas, function(x) which(x > 10000000))
for(i in 1:length(bigpolys)){
if(length(bigpolys[[i]]) >= 1 && bigpolys[[i]][1] >= 1){
bdy.can1@polygons[[i]]@Polygons <- bdy.can1@polygons[[i]]@Polygons[bigpolys[[i]]]
bdy.can1@polygons[[i]]@plotOrder <- 1:length(bdy.can1@polygons[[i]]@Polygons)
}
}
areas <- lapply(bdy.usa1@polygons, function(x) sapply(x@Polygons, function(y) y@area))
bigpolys <- lapply(areas, function(x) which(x > 10000000))
for(i in 1:length(bigpolys)){
if(length(bigpolys[[i]]) >= 1 && bigpolys[[i]][1] >= 1){
bdy.usa1@polygons[[i]]@Polygons <- bdy.usa1@polygons[[i]]@Polygons[bigpolys[[i]]]
bdy.usa1@polygons[[i]]@plotOrder <- 1:length(bdy.usa1@polygons[[i]]@Polygons)
}
}
## WWF terrestrial ecoregions of the world (for major lakes)
teow <- readShapePoly("C:\\Users\\Colin\\Documents\\Masters\\Research\\SpatialData\\WWF\\TEOW\\wwf_terr_ecos.shp")
P4S.latlon <- CRS("+proj=longlat +datum=WGS84")
projection(teow) <- P4S.latlon
teow <- spTransform(teow, P4S.NAEC)
# #####################
# #### RESULTS
# #####################
RCP <- "RCP45"
# #####################
# #### Base Case NAnaec4
# #####################
type <- "SimpleMahal"
VarSet <- "Seasonal"
VarCode <- "S"
Grid.medium <- "NAnaec8"
Grid.fine <- "NAnaec4"
Grid.map <- "dem4"
proj.year <- 2085
model <- models[9]
scenario <- "BaseCase"
description <- paste("Base Case (", RCP,")", sep="")
## DEM and land definition
setwd("C:\\Users\\Colin\\Documents\\Masters\\Research\\Data\\Novelty2016\\InputData")
dem <- raster(paste(Grid.map,".tif", sep=""))
land.medium <- read.csv(paste("land.",Grid.medium,".csv", sep=""))[,1]
land.fine <- read.csv(paste("land.",Grid.fine,".csv", sep=""))[,1]
#read in identities and distances to 4 nearest CRU neighbours for each reference grid cell.
A.grid.4nn <- read.csv(paste("grid4nn_",Grid.fine,".csv",sep=""), strip.white = TRUE, na.strings = c("NA","",-9999) )
A.grid.dist <- read.csv(paste("griddist_",Grid.fine,".csv",sep=""), strip.white = TRUE, na.strings = c("NA","",-9999) )
trunc.SD2s <- 0.1
trunc.rule2 <- paste(trunc.SD2s,"SD2",sep="")
setwd("C:\\Users\\Colin\\Documents\\Masters\\Research\\Data\\Novelty2016\\OutputData")
#import the novelty distance files for the ensemble
filelist <- list.files(pattern = paste("NNdistproj_",Grid.fine,".*.",RCP,".*.",proj.year,".*.",trunc.rule2,".*.",type,".*.",scenario,".*.",".csv", sep=""), recursive =F )
NN.dist.proj <- read.csv(filelist[grep(model,filelist)])
####Convert the NN.dist.proj data frame to chi percentiles
#step 1: import the PC standard deviation vectors for each CRU surrogate
filelist_sd <- list.files(pattern = paste("PC2sdev_",Grid.fine,".*.",RCP,".*.",proj.year,".*.",trunc.rule2,".*.",type,".*.",scenario,".*.",".csv", sep=""), recursive =F )
PC2.sdev <- read.csv(filelist_sd[grep(model,filelist_sd)])
#step 2: find the dimensionality of each cru surrogate climate space based on the truncation rule.
PC2s <- apply(PC2.sdev,1,function(x){PC <- max(which(x>trunc.SD2s)); return(PC)})
#step 3: convert to chi percentiles (i used the pchisq() function because the coding of the pchi{adehabitatLT} function runs about 100 times slower)
NN.chi.proj <- NN.dist.proj  #initiate the data frame
for(j in 1:length(NN.chi.proj)){
PC2s.long <- PC2s[match(A.grid.4nn[,j],seq(1,length(PC2s)))] #finds the number of PCs used for the necessary to facilitate the next loop.
for(k in unique(PC2s)){    #this clumsy loop is necessary becuase the pchisq function can't handle degrees of freedom in a vector format, so we need to loop through the unique degrees of freedom (dimensionality)
NN.chi.proj[which(PC2s.long==k),j] <- pchisq(NN.chi.proj[which(PC2s.long==k),j]^2,k)
}
print(j)}
#inverse distance-weighted averaging function.
IDWMean <- function(x){IDWAvg <- sum(x[1:(length(x)/2)]/x[(length(x)/2+1):length(x)])/sum(1/x[(length(x)/2+1):length(x)]); return(IDWAvg)}   #this is an awkward solution to the problem of needing a one-argument function for use by "apply". the idea is to cbind the NN.dist.proj and grid.dist data frames into one argument for the function to operate on as a single vector.
NN.chi.proj.IDWavg <- apply(cbind(NN.chi.proj,A.grid.dist),1,IDWMean) #apply inverse distance-weighted averaging function
# NN.chi.proj.IDWavg[which(A.grid.dist[,1]==0)] <- NN.chi.proj[which(A.grid.dist[,1]==0),1]   #the IDWMean function returns NaN for the cells with zero distance to their nearest neighbour. this step assigns the value for that cell, since no averaging is required.
assign(paste("NN.chi.proj.IDWavg", Grid.fine, sep="."), NN.chi.proj.IDWavg)
# #####################
# #### Base Case WNAnaec2
# #####################
type <- "SimpleMahal"
VarSet <- "Seasonal"
VarCode <- "S"
Grid.medium <- "NAnaec8"
Grid.fine <- "WNAnaec2"
Grid.map <- "dem2"
proj.year <- 2085
model <- models[9]
scenario <- "BaseCase"
description <- paste("Base Case (", RCP,")", sep="")
## DEM and land definition
setwd("C:\\Users\\Colin\\Documents\\Masters\\Research\\Data\\Novelty2016\\InputData")
dem <- raster(paste(Grid.map,".tif", sep=""))
land.medium <- read.csv(paste("land.",Grid.medium,".csv", sep=""))[,1]
land.fine <- read.csv(paste("land.",Grid.fine,".csv", sep=""))[,1]
#read in identities and distances to 4 nearest CRU neighbours for each reference grid cell.
A.grid.4nn <- read.csv(paste("grid4nn_",Grid.fine,".csv",sep=""), strip.white = TRUE, na.strings = c("NA","",-9999) )
A.grid.dist <- read.csv(paste("griddist_",Grid.fine,".csv",sep=""), strip.white = TRUE, na.strings = c("NA","",-9999) )
trunc.SD2s <- 0.1
trunc.rule2 <- paste(trunc.SD2s,"SD2",sep="")
setwd("C:\\Users\\Colin\\Documents\\Masters\\Research\\Data\\Novelty2016\\OutputData")
#import the novelty distance files for the ensemble
filelist <- list.files(pattern = paste("NNdistproj_",Grid.fine,".*.",RCP,".*.",proj.year,".*.",trunc.rule2,".*.",type,".*.",scenario,".*.",".csv", sep=""), recursive =F )
NN.dist.proj <- read.csv(filelist[grep(model,filelist)])
####Convert the NN.dist.proj data frame to chi percentiles
#step 1: import the PC standard deviation vectors for each CRU surrogate
filelist_sd <- list.files(pattern = paste("PC2sdev_",Grid.fine,".*.",RCP,".*.",proj.year,".*.",trunc.rule2,".*.",type,".*.",scenario,".*.",".csv", sep=""), recursive =F )
PC2.sdev <- read.csv(filelist_sd[grep(model,filelist_sd)])
#step 2: find the dimensionality of each cru surrogate climate space based on the truncation rule.
PC2s <- apply(PC2.sdev,1,function(x){PC <- max(which(x>trunc.SD2s)); return(PC)})
#step 3: convert to chi percentiles (i used the pchisq() function because the coding of the pchi{adehabitatLT} function runs about 100 times slower)
NN.chi.proj <- NN.dist.proj  #initiate the data frame
for(j in 1:length(NN.chi.proj)){
PC2s.long <- PC2s[match(A.grid.4nn[,j],seq(1,length(PC2s)))] #finds the number of PCs used for the necessary to facilitate the next loop.
for(k in unique(PC2s)){    #this clumsy loop is necessary becuase the pchisq function can't handle degrees of freedom in a vector format, so we need to loop through the unique degrees of freedom (dimensionality)
NN.chi.proj[which(PC2s.long==k),j] <- pchisq(NN.chi.proj[which(PC2s.long==k),j]^2,k)
}
print(j)}
#inverse distance-weighted averaging function.
IDWMean <- function(x){IDWAvg <- sum(x[1:(length(x)/2)]/x[(length(x)/2+1):length(x)])/sum(1/x[(length(x)/2+1):length(x)]); return(IDWAvg)}   #this is an awkward solution to the problem of needing a one-argument function for use by "apply". the idea is to cbind the NN.dist.proj and grid.dist data frames into one argument for the function to operate on as a single vector.
NN.chi.proj.IDWavg <- apply(cbind(NN.chi.proj,A.grid.dist),1,IDWMean) #apply inverse distance-weighted averaging function
# NN.chi.proj.IDWavg[which(A.grid.dist[,1]==0)] <- NN.chi.proj[which(A.grid.dist[,1]==0),1]   #the IDWMean function returns NaN for the cells with zero distance to their nearest neighbour. this step assigns the value for that cell, since no averaging is required.
assign(paste("NN.chi.proj.IDWavg", Grid.fine, sep="."), NN.chi.proj.IDWavg)
Grid.fine <- "WNAnaec2"
X <- dem
values(X) <- NA
values(X)[land.fine] <- round(pchi(sigma(NN.chi.proj.IDWavg,1)),2)
values(X)[land.fine] <- round(pchi(NN.chi.proj.IDWavg,1),2)
novelty <- pchi(NN.chi.proj.IDWavg,1)
novelty[which(NN.chi.proj.IDWavg==1)] <- max(novelty[is.finite(novelty)])
X <- dem
values(X) <- NA
values(X)[land.fine] <-
values(X)[land.fine] <- novelty
values(X)[land.fine] <- novelty
plot(X)
writeRaster(X, paste("SigmaNovelty", RCP, Grid.fine, sep="."), "raster")
setwd("C:\\Users\\Colin\\Documents\\Masters\\Research\\Data\\Novelty2016\\SampleCode")
writeRaster(X, paste("SigmaNovelty", RCP, Grid.fine, sep="."), "raster")
novelty <- round(pchi(NN.chi.proj.IDWavg,1),2)
novelty[which(NN.chi.proj.IDWavg==1)] <- max(novelty[is.finite(novelty)])
X <- dem
values(X) <- NA
values(X)[land.fine] <- novelty
writeRaster(X, paste("SigmaNovelty", RCP, Grid.fine, sep="."), "raster")
writeRaster(X, paste("SigmaNovelty", RCP, Grid.fine, sep="."), "raster", overwrite=TRUE)
writeRaster(X, paste("SigmaNovelty", RCP, Grid.fine, sep="."), "GTiff", overwrite=TRUE)
# #####################
# #### RESULTS
# #####################
RCP <- "RCP45"
# #####################
# #### Base Case NAnaec4
# #####################
type <- "SimpleMahal"
VarSet <- "Seasonal"
VarCode <- "S"
Grid.medium <- "NAnaec8"
Grid.fine <- "NAnaec4"
Grid.map <- "dem4"
proj.year <- 2085
model <- models[9]
scenario <- "BaseCase"
description <- paste("Base Case (", RCP,")", sep="")
## DEM and land definition
setwd("C:\\Users\\Colin\\Documents\\Masters\\Research\\Data\\Novelty2016\\InputData")
dem <- raster(paste(Grid.map,".tif", sep=""))
land.medium <- read.csv(paste("land.",Grid.medium,".csv", sep=""))[,1]
land.fine <- read.csv(paste("land.",Grid.fine,".csv", sep=""))[,1]
#read in identities and distances to 4 nearest CRU neighbours for each reference grid cell.
A.grid.4nn <- read.csv(paste("grid4nn_",Grid.fine,".csv",sep=""), strip.white = TRUE, na.strings = c("NA","",-9999) )
A.grid.dist <- read.csv(paste("griddist_",Grid.fine,".csv",sep=""), strip.white = TRUE, na.strings = c("NA","",-9999) )
trunc.SD2s <- 0.1
trunc.rule2 <- paste(trunc.SD2s,"SD2",sep="")
setwd("C:\\Users\\Colin\\Documents\\Masters\\Research\\Data\\Novelty2016\\OutputData")
#import the novelty distance files for the ensemble
filelist <- list.files(pattern = paste("NNdistproj_",Grid.fine,".*.",RCP,".*.",proj.year,".*.",trunc.rule2,".*.",type,".*.",scenario,".*.",".csv", sep=""), recursive =F )
NN.dist.proj <- read.csv(filelist[grep(model,filelist)])
####Convert the NN.dist.proj data frame to chi percentiles
#step 1: import the PC standard deviation vectors for each CRU surrogate
filelist_sd <- list.files(pattern = paste("PC2sdev_",Grid.fine,".*.",RCP,".*.",proj.year,".*.",trunc.rule2,".*.",type,".*.",scenario,".*.",".csv", sep=""), recursive =F )
PC2.sdev <- read.csv(filelist_sd[grep(model,filelist_sd)])
#step 2: find the dimensionality of each cru surrogate climate space based on the truncation rule.
PC2s <- apply(PC2.sdev,1,function(x){PC <- max(which(x>trunc.SD2s)); return(PC)})
#step 3: convert to chi percentiles (i used the pchisq() function because the coding of the pchi{adehabitatLT} function runs about 100 times slower)
NN.chi.proj <- NN.dist.proj  #initiate the data frame
for(j in 1:length(NN.chi.proj)){
PC2s.long <- PC2s[match(A.grid.4nn[,j],seq(1,length(PC2s)))] #finds the number of PCs used for the necessary to facilitate the next loop.
for(k in unique(PC2s)){    #this clumsy loop is necessary becuase the pchisq function can't handle degrees of freedom in a vector format, so we need to loop through the unique degrees of freedom (dimensionality)
NN.chi.proj[which(PC2s.long==k),j] <- pchisq(NN.chi.proj[which(PC2s.long==k),j]^2,k)
}
print(j)}
#inverse distance-weighted averaging function.
IDWMean <- function(x){IDWAvg <- sum(x[1:(length(x)/2)]/x[(length(x)/2+1):length(x)])/sum(1/x[(length(x)/2+1):length(x)]); return(IDWAvg)}   #this is an awkward solution to the problem of needing a one-argument function for use by "apply". the idea is to cbind the NN.dist.proj and grid.dist data frames into one argument for the function to operate on as a single vector.
NN.chi.proj.IDWavg <- apply(cbind(NN.chi.proj,A.grid.dist),1,IDWMean) #apply inverse distance-weighted averaging function
# NN.chi.proj.IDWavg[which(A.grid.dist[,1]==0)] <- NN.chi.proj[which(A.grid.dist[,1]==0),1]   #the IDWMean function returns NaN for the cells with zero distance to their nearest neighbour. this step assigns the value for that cell, since no averaging is required.
assign(paste("NN.chi.proj.IDWavg", Grid.fine, sep="."), NN.chi.proj.IDWavg)
################################
### Export spatial data
################################
setwd("C:\\Users\\Colin\\Documents\\Masters\\Research\\Data\\Novelty2016\\SampleCode")
novelty <- round(pchi(NN.chi.proj.IDWavg,1),2)
novelty[which(NN.chi.proj.IDWavg==1)] <- max(novelty[is.finite(novelty)])
X <- dem
values(X) <- NA
values(X)[land.fine] <- novelty
writeRaster(X, paste("SigmaNovelty", RCP, Grid.fine, sep="."), "GTiff", overwrite=TRUE)
# #####################
# #### RESULTS
# #####################
RCP <- "RCP85"
# #####################
# #### Base Case NAnaec4
# #####################
type <- "SimpleMahal"
VarSet <- "Seasonal"
VarCode <- "S"
Grid.medium <- "NAnaec8"
Grid.fine <- "NAnaec4"
Grid.map <- "dem4"
proj.year <- 2085
model <- models[9]
scenario <- "BaseCase"
description <- paste("Base Case (", RCP,")", sep="")
## DEM and land definition
setwd("C:\\Users\\Colin\\Documents\\Masters\\Research\\Data\\Novelty2016\\InputData")
dem <- raster(paste(Grid.map,".tif", sep=""))
land.medium <- read.csv(paste("land.",Grid.medium,".csv", sep=""))[,1]
land.fine <- read.csv(paste("land.",Grid.fine,".csv", sep=""))[,1]
#read in identities and distances to 4 nearest CRU neighbours for each reference grid cell.
A.grid.4nn <- read.csv(paste("grid4nn_",Grid.fine,".csv",sep=""), strip.white = TRUE, na.strings = c("NA","",-9999) )
A.grid.dist <- read.csv(paste("griddist_",Grid.fine,".csv",sep=""), strip.white = TRUE, na.strings = c("NA","",-9999) )
trunc.SD2s <- 0.1
trunc.rule2 <- paste(trunc.SD2s,"SD2",sep="")
setwd("C:\\Users\\Colin\\Documents\\Masters\\Research\\Data\\Novelty2016\\OutputData")
#import the novelty distance files for the ensemble
filelist <- list.files(pattern = paste("NNdistproj_",Grid.fine,".*.",RCP,".*.",proj.year,".*.",trunc.rule2,".*.",type,".*.",scenario,".*.",".csv", sep=""), recursive =F )
NN.dist.proj <- read.csv(filelist[grep(model,filelist)])
####Convert the NN.dist.proj data frame to chi percentiles
#step 1: import the PC standard deviation vectors for each CRU surrogate
filelist_sd <- list.files(pattern = paste("PC2sdev_",Grid.fine,".*.",RCP,".*.",proj.year,".*.",trunc.rule2,".*.",type,".*.",scenario,".*.",".csv", sep=""), recursive =F )
PC2.sdev <- read.csv(filelist_sd[grep(model,filelist_sd)])
#step 2: find the dimensionality of each cru surrogate climate space based on the truncation rule.
PC2s <- apply(PC2.sdev,1,function(x){PC <- max(which(x>trunc.SD2s)); return(PC)})
#step 3: convert to chi percentiles (i used the pchisq() function because the coding of the pchi{adehabitatLT} function runs about 100 times slower)
NN.chi.proj <- NN.dist.proj  #initiate the data frame
for(j in 1:length(NN.chi.proj)){
PC2s.long <- PC2s[match(A.grid.4nn[,j],seq(1,length(PC2s)))] #finds the number of PCs used for the necessary to facilitate the next loop.
for(k in unique(PC2s)){    #this clumsy loop is necessary becuase the pchisq function can't handle degrees of freedom in a vector format, so we need to loop through the unique degrees of freedom (dimensionality)
NN.chi.proj[which(PC2s.long==k),j] <- pchisq(NN.chi.proj[which(PC2s.long==k),j]^2,k)
}
print(j)}
#inverse distance-weighted averaging function.
IDWMean <- function(x){IDWAvg <- sum(x[1:(length(x)/2)]/x[(length(x)/2+1):length(x)])/sum(1/x[(length(x)/2+1):length(x)]); return(IDWAvg)}   #this is an awkward solution to the problem of needing a one-argument function for use by "apply". the idea is to cbind the NN.dist.proj and grid.dist data frames into one argument for the function to operate on as a single vector.
NN.chi.proj.IDWavg <- apply(cbind(NN.chi.proj,A.grid.dist),1,IDWMean) #apply inverse distance-weighted averaging function
# NN.chi.proj.IDWavg[which(A.grid.dist[,1]==0)] <- NN.chi.proj[which(A.grid.dist[,1]==0),1]   #the IDWMean function returns NaN for the cells with zero distance to their nearest neighbour. this step assigns the value for that cell, since no averaging is required.
assign(paste("NN.chi.proj.IDWavg", Grid.fine, sep="."), NN.chi.proj.IDWavg)
################################
### Export spatial data
################################
setwd("C:\\Users\\Colin\\Documents\\Masters\\Research\\Data\\Novelty2016\\SampleCode")
novelty <- round(pchi(NN.chi.proj.IDWavg,1),2)
novelty[which(NN.chi.proj.IDWavg==1)] <- max(novelty[is.finite(novelty)])
X <- dem
values(X) <- NA
values(X)[land.fine] <- novelty
writeRaster(X, paste("SigmaNovelty", RCP, Grid.fine, sep="."), "GTiff", overwrite=TRUE)
# #####################
# #### Base Case WNAnaec2
# #####################
type <- "SimpleMahal"
VarSet <- "Seasonal"
VarCode <- "S"
Grid.medium <- "NAnaec8"
Grid.fine <- "WNAnaec2"
Grid.map <- "dem2"
proj.year <- 2085
model <- models[9]
scenario <- "BaseCase"
description <- paste("Base Case (", RCP,")", sep="")
## DEM and land definition
setwd("C:\\Users\\Colin\\Documents\\Masters\\Research\\Data\\Novelty2016\\InputData")
dem <- raster(paste(Grid.map,".tif", sep=""))
land.medium <- read.csv(paste("land.",Grid.medium,".csv", sep=""))[,1]
land.fine <- read.csv(paste("land.",Grid.fine,".csv", sep=""))[,1]
#read in identities and distances to 4 nearest CRU neighbours for each reference grid cell.
A.grid.4nn <- read.csv(paste("grid4nn_",Grid.fine,".csv",sep=""), strip.white = TRUE, na.strings = c("NA","",-9999) )
A.grid.dist <- read.csv(paste("griddist_",Grid.fine,".csv",sep=""), strip.white = TRUE, na.strings = c("NA","",-9999) )
trunc.SD2s <- 0.1
trunc.rule2 <- paste(trunc.SD2s,"SD2",sep="")
setwd("C:\\Users\\Colin\\Documents\\Masters\\Research\\Data\\Novelty2016\\OutputData")
#import the novelty distance files for the ensemble
filelist <- list.files(pattern = paste("NNdistproj_",Grid.fine,".*.",RCP,".*.",proj.year,".*.",trunc.rule2,".*.",type,".*.",scenario,".*.",".csv", sep=""), recursive =F )
NN.dist.proj <- read.csv(filelist[grep(model,filelist)])
####Convert the NN.dist.proj data frame to chi percentiles
#step 1: import the PC standard deviation vectors for each CRU surrogate
filelist_sd <- list.files(pattern = paste("PC2sdev_",Grid.fine,".*.",RCP,".*.",proj.year,".*.",trunc.rule2,".*.",type,".*.",scenario,".*.",".csv", sep=""), recursive =F )
PC2.sdev <- read.csv(filelist_sd[grep(model,filelist_sd)])
#step 2: find the dimensionality of each cru surrogate climate space based on the truncation rule.
PC2s <- apply(PC2.sdev,1,function(x){PC <- max(which(x>trunc.SD2s)); return(PC)})
#step 3: convert to chi percentiles (i used the pchisq() function because the coding of the pchi{adehabitatLT} function runs about 100 times slower)
NN.chi.proj <- NN.dist.proj  #initiate the data frame
for(j in 1:length(NN.chi.proj)){
PC2s.long <- PC2s[match(A.grid.4nn[,j],seq(1,length(PC2s)))] #finds the number of PCs used for the necessary to facilitate the next loop.
for(k in unique(PC2s)){    #this clumsy loop is necessary becuase the pchisq function can't handle degrees of freedom in a vector format, so we need to loop through the unique degrees of freedom (dimensionality)
NN.chi.proj[which(PC2s.long==k),j] <- pchisq(NN.chi.proj[which(PC2s.long==k),j]^2,k)
}
print(j)}
#inverse distance-weighted averaging function.
IDWMean <- function(x){IDWAvg <- sum(x[1:(length(x)/2)]/x[(length(x)/2+1):length(x)])/sum(1/x[(length(x)/2+1):length(x)]); return(IDWAvg)}   #this is an awkward solution to the problem of needing a one-argument function for use by "apply". the idea is to cbind the NN.dist.proj and grid.dist data frames into one argument for the function to operate on as a single vector.
NN.chi.proj.IDWavg <- apply(cbind(NN.chi.proj,A.grid.dist),1,IDWMean) #apply inverse distance-weighted averaging function
# NN.chi.proj.IDWavg[which(A.grid.dist[,1]==0)] <- NN.chi.proj[which(A.grid.dist[,1]==0),1]   #the IDWMean function returns NaN for the cells with zero distance to their nearest neighbour. this step assigns the value for that cell, since no averaging is required.
assign(paste("NN.chi.proj.IDWavg", Grid.fine, sep="."), NN.chi.proj.IDWavg)
################################
### Export spatial data
################################
setwd("C:\\Users\\Colin\\Documents\\Masters\\Research\\Data\\Novelty2016\\SampleCode")
novelty <- round(pchi(NN.chi.proj.IDWavg,1),2)
novelty[which(NN.chi.proj.IDWavg==1)] <- max(novelty[is.finite(novelty)])
X <- dem
values(X) <- NA
values(X)[land.fine] <- novelty
writeRaster(X, paste("SigmaNovelty", RCP, Grid.fine, sep="."), "GTiff", overwrite=TRUE)
# #####################
# #### RESULTS
# #####################
RCP <- "RCP85"
# #####################
# #### Base Case NAnaec4
# #####################
type <- "SimpleMahal"
VarSet <- "Seasonal"
VarCode <- "S"
Grid.medium <- "NAnaec8"
Grid.fine <- "NAnaec4"
Grid.map <- "dem4"
proj.year <- 2085
model <- models[9]
scenario <- "BaseCase"
description <- paste("Base Case (", RCP,")", sep="")
## DEM and land definition
setwd("C:\\Users\\Colin\\Documents\\Masters\\Research\\Data\\Novelty2016\\InputData")
dem <- raster(paste(Grid.map,".tif", sep=""))
land.medium <- read.csv(paste("land.",Grid.medium,".csv", sep=""))[,1]
land.fine <- read.csv(paste("land.",Grid.fine,".csv", sep=""))[,1]
#read in identities and distances to 4 nearest CRU neighbours for each reference grid cell.
A.grid.4nn <- read.csv(paste("grid4nn_",Grid.fine,".csv",sep=""), strip.white = TRUE, na.strings = c("NA","",-9999) )
A.grid.dist <- read.csv(paste("griddist_",Grid.fine,".csv",sep=""), strip.white = TRUE, na.strings = c("NA","",-9999) )
trunc.SD2s <- 0.1
trunc.rule2 <- paste(trunc.SD2s,"SD2",sep="")
setwd("C:\\Users\\Colin\\Documents\\Masters\\Research\\Data\\Novelty2016\\OutputData")
#import the novelty distance files for the ensemble
filelist <- list.files(pattern = paste("NNdistproj_",Grid.fine,".*.",RCP,".*.",proj.year,".*.",trunc.rule2,".*.",type,".*.",scenario,".*.",".csv", sep=""), recursive =F )
NN.dist.proj <- read.csv(filelist[grep(model,filelist)])
####Convert the NN.dist.proj data frame to chi percentiles
#step 1: import the PC standard deviation vectors for each CRU surrogate
filelist_sd <- list.files(pattern = paste("PC2sdev_",Grid.fine,".*.",RCP,".*.",proj.year,".*.",trunc.rule2,".*.",type,".*.",scenario,".*.",".csv", sep=""), recursive =F )
PC2.sdev <- read.csv(filelist_sd[grep(model,filelist_sd)])
#step 2: find the dimensionality of each cru surrogate climate space based on the truncation rule.
PC2s <- apply(PC2.sdev,1,function(x){PC <- max(which(x>trunc.SD2s)); return(PC)})
#step 3: convert to chi percentiles (i used the pchisq() function because the coding of the pchi{adehabitatLT} function runs about 100 times slower)
NN.chi.proj <- NN.dist.proj  #initiate the data frame
for(j in 1:length(NN.chi.proj)){
PC2s.long <- PC2s[match(A.grid.4nn[,j],seq(1,length(PC2s)))] #finds the number of PCs used for the necessary to facilitate the next loop.
for(k in unique(PC2s)){    #this clumsy loop is necessary becuase the pchisq function can't handle degrees of freedom in a vector format, so we need to loop through the unique degrees of freedom (dimensionality)
NN.chi.proj[which(PC2s.long==k),j] <- pchisq(NN.chi.proj[which(PC2s.long==k),j]^2,k)
}
print(j)}
#inverse distance-weighted averaging function.
IDWMean <- function(x){IDWAvg <- sum(x[1:(length(x)/2)]/x[(length(x)/2+1):length(x)])/sum(1/x[(length(x)/2+1):length(x)]); return(IDWAvg)}   #this is an awkward solution to the problem of needing a one-argument function for use by "apply". the idea is to cbind the NN.dist.proj and grid.dist data frames into one argument for the function to operate on as a single vector.
NN.chi.proj.IDWavg <- apply(cbind(NN.chi.proj,A.grid.dist),1,IDWMean) #apply inverse distance-weighted averaging function
# NN.chi.proj.IDWavg[which(A.grid.dist[,1]==0)] <- NN.chi.proj[which(A.grid.dist[,1]==0),1]   #the IDWMean function returns NaN for the cells with zero distance to their nearest neighbour. this step assigns the value for that cell, since no averaging is required.
assign(paste("NN.chi.proj.IDWavg", Grid.fine, sep="."), NN.chi.proj.IDWavg)
################################
### Export spatial data
################################
setwd("C:\\Users\\Colin\\Documents\\Masters\\Research\\Data\\Novelty2016\\SampleCode")
novelty <- round(pchi(NN.chi.proj.IDWavg,1),2)
Grid.fine
Y <- raster(paste(paste("SigmaNovelty", RCP, Grid.fine, "tif", sep="."))
Y <- raster(paste("SigmaNovelty", RCP, Grid.fine, "tif", sep="."))
Y <- raster(paste("SigmaNovelty", RCP, Grid.fine, "tif", sep="."))
plot(Y)
projection(dem)
